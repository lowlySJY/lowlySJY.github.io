<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Notes for Deep Reinforcement Learning</title>
      <link href="2022/03/17/notes-for-deep-reinforcement-learning/"/>
      <url>2022/03/17/notes-for-deep-reinforcement-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Fundamentals"><a href="#Fundamentals" class="headerlink" title="Fundamentals"></a>Fundamentals</h1><p>DL has a powerful data representation ability but it is not enough to build a smart AI system. This is because an AI system should not only able to learn from the provided data but also able to learn from interactions with the real world environment like a human. </p><p>RL is a subset of ML that enables computers to learn by interacting with the real world environment. In brief, RL separates the real world into two components—an environment and an agent. The agent interacts with the environment by performing specific actions and receives feedback from the environment. The feedback is usually termed as the “reward” in RL. The agent learns to perform “better” by trying to get more positive rewards from the environment. This learning process forms a feedback loop between the environment and agent, guiding the improvement of the agent with RL algorithms.</p><p>DRL is to combine the advantages of DL and RL for building AI systems. The main reason to use DL in RL is to leverage the scalability of DNN in high-dimensional space.</p><h2 id="Introduction-to-Deep-learning"><a href="#Introduction-to-Deep-learning" class="headerlink" title="Introduction to Deep learning"></a>Introduction to Deep learning</h2><p><strong>Discriminative Models</strong> study the conditional probability p(y|x) with input data x and a target label y. In other words, discriminative models predict the label y given the input data x. Discriminative models are mostly adopted in tasks such as classification and regression which require discriminative judgements.</p><p><strong>Generative Models</strong> are designed to study the joint probability p(x, y). Generative models are usually used to generate observed data by learning the distribution of the observed data.</p><h2 id="Introduction-to-Reinforcement-Learning"><a href="#Introduction-to-Reinforcement-Learning" class="headerlink" title="Introduction to Reinforcement Learning"></a>Introduction to Reinforcement Learning</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>The <strong>agent</strong> (智能体) and <strong>environment</strong> (环境) are the basic components of reinforcement learning.</p><p>The environment is an entity that the agent can interact with. An agent can “interact” with the environment by using a predefined <strong>action set</strong> <em>A<del>t</del></em> that describes all possible actions and get <strong>reward</strong> feedback by environment. The goal of reinforcement learning algorithms is to teach the agent how to interact “well” with the environment so that the agent is able to obtain a good score under a predefined evaluation metric.</p><p>At an arbitrary time step, <em>t</em>, the agent first observes the current state of the environment, <em>S<del>t</del></em> , and the corresponding reward value, <em>R<del>t</del></em> . The agent then decides what to do next based on the state and reward information. The action the agent intends to perform, <em>A<del>t</del></em> , gets fed back into the environment such that we can obtain the new state <em>S<del>t+1</del></em> and reward <em>R<del>t+1</del></em>. The observation of the environment state <em>s</em> (<em>s</em> is a general representation of state regardless of time step s) from the agent’s perspective does not always contain all the information about the environment. If the observation only contains partial state information, the environment is <strong>partially observable</strong>. Nevertheless, if the observation contains the complete state information of the environment, the environment is <strong>fully observable</strong>.</p><p>To provide feedback from the environment to the agent, a <strong>reward function</strong> <em>R</em> generates an <strong>immediate reward</strong> <em>R<del>t</del></em> according to the environment status and sends it to the agent at every time step. The reward function can depend on the current state only or the sequence of state-action pairs. In reinforcement learning, a <strong>trajectory</strong> (轨迹) is a sequence of states, actions, and rewards:<br>$$<br>\tau = (S_0, A_0, R_0, S_1, A_1, R_1,…)<br>$$<br>which records how the agent interacts with the environment. The initial state in a trajectory, <em>S<del>0</del></em>, is randomly sampled from the <strong>start-state distribution</strong>, denoted by <em>ρ<del>0</del></em>, in which:<br>$$<br>S_0 \sim \rho_0(\cdot)<br>$$<br>The transition from a state to the next state can be either a <strong>deterministic transition process</strong> or a <strong>stochastic transition process</strong>. For the deterministic transition, the next state St+1 is governed by a deterministic function:<br>$$<br>S_{t+1} = f(S_t, A_t)<br>$$<br>where a unique next state St+1 can be found. For the stochastic transition process, the next state St+1 is described as a probabilistic distribution:<br>$$<br>S_{t+1} \sim p(S_{t+1}|S_t, A_t)<br>$$<br>A trajectory, being referred to also as an <strong>episode</strong> (片段) , is a sequence that goes from the initial state to the terminal state (for finite cases).</p><p><strong>Exploitation</strong> (利用，也称为守成) means maximizing the agent performance using the existing knowledge, and its performance is usually evaluated by the expected reward. The <strong>greedy policy</strong> means the agent constantly performs the action that yields the highest expected reward based on current information, rather than taking risky trials which may lead to lower expected rewards.</p><p><strong>Exploration</strong> means increasing existing knowledge by taking actions and interacting with the environment. The <strong>exploration-exploitation trade-off</strong> describes the balance between how much efforts the agent makes on exploration and exploitation, respectively. The trade-off between exploration and exploitation is a central theme of reinforcement learning research and reinforcement learning algorithm development. </p><h3 id="Online-Prediction-and-Online-Learning"><a href="#Online-Prediction-and-Online-Learning" class="headerlink" title="Online Prediction and Online Learning"></a>Online Prediction and Online Learning</h3><p><strong>Online prediction problems</strong> are the class of problems where the agent has to make predictions about the future. Online prediction problems need to be solved with online methods. Online learning is distinguished from traditional statistic learning in the following aspects:</p><ul><li>The sample data is presented in an ordered sequence instead of an unordered batch.</li><li>We will often have to consider the worst case rather than the average case because we do not want things to go out of control in the learning stage</li><li>The learning objective can be different as online learning often tries to minimize regret whereas statistical learning tries to minimize empirical risk.</li></ul><p>The regret after n steps is defined as:<br>$$<br>R E_n = \left({\underset{j=1,2,…,K} {max}} {\sum_{t=1}^n}R_t^j\right)-{\sum_{t=1}^n}R_t^i<br>$$<br>The first term in the subtraction is the total reward that we accumulate until time <em>n</em> for always receiving the maximized rewards and the second term is the actual accumulated rewards in a trial that has gone through <em>n</em> time steps. In order to select the best action, we should try to minimize the expected regret because of the stochasticity introduced by our actions and rewards. We will differentiate two different types of regret, the expected regret and pseudo-regret (伪后悔值). The expected regret is defined as:<br>$$<br>\Bbb{E}[RE_n] = \Bbb{E}\left[({\underset{j=1,2,…,K} {max}} {\sum_{t=1}^n}R_t^j)-{\sum_{t=1}^n}R_t^i\right]<br>$$<br>The pseudo-regret is defined as:<br>$$<br>\overline {R E_n} = {\underset{j=1,2,…,K} {max}} \Bbb{E}\left[ {\sum_{t=1}^n}R_t^j-{\sum_{t=1}^n}R_t^i \right]<br>$$<br>The expected regret is harder to compute. This is because for the pseudo-regret we only need to find the action that optimizes the regret in expectation, however, for the expected regret, we will have to find the expected  regret that is optimal over actions across different trials. Concretely, we have $$\Bbb{E}[REn] ≥ \overline{REn}$$.</p><p>Stochastic MAB’s (Multi-Armed Bandit) reward functions are determined by the probabilistic distributions that are usually not changed. In reality, this might not be the case. Adversarial MAB modeling is needed when the rewards are no longer governed by a stationary probabilistic distributions but arbitrarily determined by some <strong>adversary</strong> (对抗者).</p><p>The general setup for adversarial MAB: At each time step, the agent will choose an arm <em>I<del>t</del></em> to pull and the adversary will decide the reward vector <em>R<del>t</del></em> for this time step. </p><p>We will call the adversary who sets the rewards independent of the past history an <strong>oblivious adversary</strong> (健忘对抗者) and the one that sets the rewards based on the past history a non-<strong>oblivious adversary</strong> (非健忘对抗者). We call the game in which a player has full knowledge of the reward vector a <strong>full-information game</strong> (全信息博弈) and the game with the knowledge of the reward for the action being played a <strong>partial-information game</strong> (部分信息博弈).</p><p>When the reward function for the task is stationary, we only need to find the best action, otherwise, when the task is non-stationary we will try to keep track of the changes.</p><h3 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h3><h4 id="Markov-Process-MP"><a href="#Markov-Process-MP" class="headerlink" title="Markov Process (MP)"></a>Markov Process (MP)</h4><p> A Markov process (MP) is a discrete stochastic process with Markov property, which simplifies the simulation of the world in continuous space. Figure 2.4 shows an example of MP. Each circle represents a state and each edge represents a state transition (状态转移)</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220323163708.bmp" alt="Figure2.4"></p><p>In <strong>probabilistic graphical models</strong> (概率图模型) , specifically the ones that we use in this book, a circle indicates a variable, and the arrow with a single direction indicates the relationship between two variables.  For example, “a → b” indicates that variable b is dependent on variable a. The variable in a circle in white denotes a normal variable, while the variable in a circle with a shade of gray denotes an observed variable. A solid black square with variables inside indicates those variables are iterative</p><p>MP follows the assumption of <strong>Markov chain</strong> where the next state <em>S<del>t+1</del></em> is only dependent on the current state <em>S<del>t</del></em> , with the probability of a state jumping to the next state described as follows:<br>$$<br>p(S_{t+1}|S_t) = p(S_{t+1}|S_0, S_1, S_2,…, S_t)<br>$$<br>We also frequently use <em>s’</em> to represent the next state, in which the probability that state <em>s</em> at time <em>t</em> will lead to state <em>s’</em> at time <em>t + 1</em> is as following in a <strong>time-homogeneous</strong> Markov chain (时间同质马尔可夫链):<br>$$<br>p(s’|s) = p(S_{t+1}=s’|S_t=s)=p(S_{t+2}=s’|S_{t+1}=s)<br>$$<br><em>The time-homogeneous property is a basic assumption for most of the derivations in the book</em>, and we will not mention it but follow it as default in most cases. However, in practice, the time-homogeneous may not always hold, especially for non-stationary environments, multi-agent reinforcement learning, etc., which concerns with time-inhomogeneous/non-homogeneous cases.</p><p>Given a finite <strong>state set</strong> <em>S</em>, we can have a <strong>state transition matrix</strong> <em>P</em>. The <em>P</em> for Fig. 2.4 is as follows:</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220323170109.bmp" alt="P"></p><p>where <em>P<del>i,j</del></em> represents the probability of transferring the current state <em>S<del>i</del></em> to the next state <em>S<del>j</del></em> . The sum of each row is equal to 1 and the <em>P</em> is always a square matrix. These probabilities indicate the whole process is stochastic. Markov process can be represented by a tuple of &lt; *S,P* &gt;. Mathematically, the next state is sampled from P as follows:<br>$$<br>S_{t+1} \sim \bf{P}_{S_t}<br>$$</p><h4 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h4><p><strong>Markov reward process</strong> (MRP) extends MP from &lt; *S,P* &gt; to &lt; *S,P, R, γ* &gt;. The R and γ represent the reward function and reward discount factor, respectively. The Fig. 2.6 and Fig. 2.7 show that the reward function depends on the current state in MRP:<br>$$<br>R_t = R(S_t)<br>$$<br> Given a list of immediate reward r for each time step in a single trajectory τ , a return is the cumulative reward of a trajectory:<br>$$<br>G_{t=0:T}=R(\tau)=\sum_{t=0}^T R_t<br>$$<br>where <em>R<del>t</del></em> is the immediate reward at time step <em>t</em>, and <em>T</em> represents the time step of the terminal state, or the total number of steps in a finite episode. For example, the trajectory (g, t<del>1</del>, t<del>2</del>, p, b) has an <strong>undiscounted return</strong> (非折扣化的回报) of 5 = −1 − 2 − 2 + 10.</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220324131804.bmp" alt="Fig2.6&amp;2.7"></p><p>Often, the steps that are closer have a greater impact than the distant ones. We introduce the concept of <strong>discounted return</strong> (折扣化回报). The discounted return is a weighted sum of rewards which gives more weights to the closer time steps. We define the discounted return as follows:<br>$$<br>G_{t=0:T}=R(\tau)=\sum_{t=0}^T \gamma^tR_t<br>$$<br>where a reward discount factor γ ∈ [0, 1] is used to reduce the weights as the time step increases. . For example in Fig. 2.6, given γ = 0.9, the trajectory (g, t<del>1</del>, t<del>2</del>, p, b) has a return of 2.87 = −1 − 2 × 0.9 − 2 × 0.9^2 + 10 × 0.9^3</p><p>The <strong>value function</strong> (价值函数) <em>V (s)</em> represents the <strong>expected return (期望回报) from the state</strong> <em>s</em>.  If the agent acts according to the policy <em>π</em>, we denote the value function as <em>$$V^{π}(s)$$</em>:<br>$$<br>V^\pi(s)=\Bbb{E}_{\tau \sim \pi}[R{(\tau)}|S_0=s]<br>$$<br>A simple way to estimate the <em>V(s)</em> is <strong>Monte Carlo method</strong> (蒙特卡罗法), we can randomly sample a large number of trajectories starting from state s according to the given state transition matrix <em>P</em>. Take Fig. 2.6 as an example, given γ = 0.9 and P, to estimate <em>$$V^{π}(s=t_2)$$</em>,  we can randomly sample four trajectories as follows and compute the returns of all trajectories individually:<br>$$<br>s=(t_2,b),R=-2+0<em>0.9=-2\<br>s=(t_2,p,b),R=-2+10</em>0.9+0<em>0.9^2=7\<br>s=(t_2,r,t_2,p,b),R=-2+1</em>0.9-20.9^2+10<em>0.9^3+0=4.57\<br>s=(t_2,r,t_1,t_2,b),R=-2+0.9-2</em>0.9^2-2*0.9^3+0=-0.178<br>$$<br>Given the returns of all trajectories, the estimated expected return under state <em>s = t<del>2</del></em> is <em>V(s = t<del>2</del>)</em> = (−2 + 7 + 4.57 − 0.178)/4 = 2.348</p><h4 id="Markov-Decision-Process-MDP"><a href="#Markov-Decision-Process-MDP" class="headerlink" title="Markov Decision Process (MDP)"></a>Markov Decision Process (MDP)</h4><p>To model the process of sequential decision making, MDP is better than MR and MRP.  As Fig. 2.9 shows, different from MRP that the immediate rewards are conditioned on the state only (reward values on nodes), the immediate rewards of MDP are associated with the action and state (reward values on edges).</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220324141411.bmp" alt="Fig. 2.9"></p><p>Given an action under a state, the next state is not fixed. For example, if the agent acts “rest” under state <em>s = t<del>2</del></em>, the next state can be either <em>s = t<del>1</del></em> or <em>t<del>2</del></em>. As mentioned above, MP can be defined as the tuple &lt; *S,P* &gt;, and MRP is defined as the tuple &lt; *S,P, R, γ* &gt;, where the element of state transition matrix is $$P_{s,s’} = p(s’ |s)$$. Here, MDP is defined as a tuple of &lt; *S, A,P, R, γ* &gt;. The element of state transition matrix becomes and Fig. 2.10 shows the graphical model of MDP in a probabilistic inference view.:<br>$$<br>p(s’|s,a) = p(S_{t+1}=s’|S_t=s,A_t=a)<br>$$<br><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220324144200.bmp" alt="Fig. 2.10"></p><p>For instance, most of the edges in Fig. 2.9 have a state transition probability of one, e.g., $$p(s’ = t_2|s = t_1, a = work) = 1$$, except that $$p(s’ |s = t_2, a = rest) = [0.2, 0.8]$$ which means if the agent performs action <em>a = rest</em> at state <em>s = t<del>2</del></em>, it has 0.2 probability will transit to state <em>s’ = t<del>1</del></em> , and 0.8 probability will keep the current state.</p><p>A <strong>policy</strong> <em>π</em> represents the way in which the agent behaves based on its observations of the environment. Specifically, the policy is a mapping from the each state s ∈ S and action a ∈ A to the probability distribution π(a|s) for taking action a in state s, where the distribution is:<br>$$<br>\pi(a|s)=p(A_t=a|S_t=s),\exist t<br>$$<br><strong>Expected return</strong> is the expectation of returns over all possible trajectories under a policy. Therefore, <strong>the goal of reinforcement learning is to find the higher expected return by optimizing the policy</strong>. Mathematically, given the start-state distribution <em>ρ<del>0</del></em> and the policy π, the probability of a T-step trajectory for MDP is:<br>$$<br>p(\tau|\pi)=\rho_0(S_0)\prod_{t=0}^{T-1}p(S_{t+1}|S_t,A_t)\pi(A_t|S_t)<br>$$<br>Given the reward function R and all possible trajectorites <em>τ</em>, the <strong>expected return</strong> J(π) is defined as follows:<br>$$<br>J(\pi)=\int_\tau p(\tau|\pi)R(\tau)=\Bbb{E}<em>{\tau\sim\pi}[R(\tau)]<br>$$<br>where <em>p</em> here means that the trajectory with higher probability will have a higher weight to the expected return. The <strong>RL optimization problem</strong> is to improve the policy for maximizing the expected return with optimization methods. The <strong>optimal policy</strong> π∗ can be expressed as:<br>$$<br>\pi^*=arg\max_\pi J(\pi)<br>$$<br>Given policy π, the <strong>value function</strong> <em>V(s)</em>, the expected return under the state, can be defined as:<br>$$<br>V^\pi(s)=\Bbb{E}</em>{\tau\sim\pi}[R(\tau)|S_0=s]\<br>        =\Bbb{E}<em>{A_t\sim\pi(\cdot|S_t)}\left[\sum</em>{t=0}^\infty\gamma^tR(S_t,A_t)|S_0=s\right]<br>$$<br>where τ ∼ π means the trajectories τ are sampled given the policy π, At ∼ π(·|St) means the action under a state is sampled from the policy, the next state is determined by the state transition matrix <strong>P</strong> given state s and action a.</p><p>In MDP, given an action, we have the <strong>action-value function</strong>, which depends on both the state and the action just taken. It gives an expected return under a state and an action. If the agent acts according to a policy π, we denote it as $$Q^π (s, a)$$, which is defined as:<br>$$<br>Q^\pi(s,a)=\Bbb{E}<em>{\tau\sim\pi}[R(\tau)|S_0=s,A_0=a]\<br>        =\Bbb{E}</em>{A_t\sim\pi(\cdot|S_t)}\left[\sum_{t=0}^\infty\gamma^tR(S_t,A_t)|S_0=s,A_0=a\right]<br>$$<br>We need to keep in mind that the $$Q^π (s, a)$$ depends on π, as the estimation of the value is an expectation over the trajectories by the policy π. This also indicates if the π changes, the corresponding $$Q^π (s, a)$$ will also change accordingly. We therefore usually call the value function estimated with a specific policy the <strong>on-policy value function</strong> (在线价值函数), for the distinction from the optimal value function estimated with the <strong>optimal policy</strong> (最优价值函数). We can observe the relation between vπ (s) and qπ (s, a):<br>$$<br>q_\pi(s,a)=\Bbb{E}<em>{\tau\sim\pi}[R(\tau)|S_0=s,A_0=a]\<br>\nu_\pi(s)=\Bbb{E}</em>{a\sim\pi}[q_\pi(s,a)]<br>$$<br>There are two simple ways to compute the value function $$v_π (s)$$ and action-value function $$q_π (s, a)$$: The first is the <strong>exhaustive method</strong> follows Eq. (40), it first computes the probabilities of all possible trajectories that start from a state, and then follows Eqs. (41) and (42) to compute the $$V ^π (s)$$ and $$Q^π (s, a)$$ for this state. However, in practice, the number of possible trajectories would be large and even infinite. Instead of using all possible trajectories, we can use <strong>Monte Carlo method</strong> as described in the previous MRP section to estimate the $$V^π (s)$$ by randomly sampling a large number of trajectories.</p><h4 id="Bellman-Equation-and-Optimality"><a href="#Bellman-Equation-and-Optimality" class="headerlink" title="Bellman Equation and Optimality"></a>Bellman Equation and Optimality</h4><p>The Bellman equation, also known as the Bellman expectation equation, is used to compute the expectation of value function given policy π, over the sampled trajectories guided by the policy. We call this <strong>“on-policy”</strong> manner as in reinforcement learning the policy is usually changing, and the value function is conditioned on or estimated by current policy</p><p>We can derive the Bellman equation for <strong>on-policy state-value function</strong> (在线状态价值函数) for MDP process in a recursive relationship:<br>$$<br>\nu_\pi(s)=\Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R(\tau_t:T)|S_t=s]\<br>=\Bbb{E}</em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R_t + \gamma R(\tau_{t+1}:T)|S_t=s]\<br>=\Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[r+\gamma\nu_\pi(s’)]<br>$$<br>the Bellman equation for MRP can be derived by simply removing the action from it:<br>$$<br>\nu(s)=\Bbb{E}</em>{s’\sim p(\cdot|s,a)}[r+\gamma\nu_(s’)]<br>$$<br>Bellman equation for **on-policy action-value function **(在线动作价值函数):<br>$$<br>q_\pi(s,a)=\Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R(\tau_t:T)|S_t=s,A_t=a]\<br>=\Bbb{E}</em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{T-1}R_T)|S_t=s,A_t=a]\<br>=\Bbb{E}<em>{S</em>{t+1}\sim\sim p(\cdot|S_{t},A_{t})}[R_{t}+\gamma \Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R(\tau</em>{t+1}:T)]|S_t=s]\<br>=\Bbb{E}<em>{S</em>{t+1}\sim\sim p(\cdot|S_{t},A_{t})}[R_{t}+\gamma \Bbb{E}<em>{A</em>{t+1}\sim\pi(\cdot|S_{t+1})}[q_\pi (S_{t+1},A_{t+1})]|S_t=s]\<br>=\Bbb{E}<em>{s’\sim p(\cdot|s,a)}[R(s,a)+\gamma \Bbb{E}</em>{a’\sim(\pi(\cdot|s’)}[q_\pi(s’,a’)]<br>$$<br>The above derivation is based on the finite MDP with maximal length of T , however, these formulas still hold when in the infinite MDP, simply with T replaced by “∞”. The two Bellman equations also do not depend on the formats of policy, which means they work for both stochastic policies π(·|s) and deterministic policies π(s).</p><p>The Bellman equation for MRP as in Eq. (47) can be solved directly if the transition function/matrix is known, which is called the <strong>inverse matrix method</strong> (逆矩阵方法). We rewrite Eq. (47) in a vector form for cases with discrete and finite state space as:<br>$$<br>\vec{\nu}=\vec{r}+\gamma\vec{P}\vec{\nu}<br>$$<br>where <strong>v</strong> and <strong>r</strong> are vectors with their elements <em>v(s)</em> and <em>R(s)</em> for all s ∈ S, and <strong>P</strong> is the transition probability matrix with elements p(s‘|s) for all s, s’ ∈ S. then<br>$$<br>\vec{\nu}=(I-\gamma\vec{P})^{-1} \vec{r}<br>$$<br>the complexity of the solution is O(n^3), where n is the number of states. Therefore this method does not work for a large number of states, meaning it may not be feasible for large-scale or continuous-valued problems. </p><p>Since on-policy value functions are estimated with respect to the policy itself, different policies will lead to different value functions, even for the same set of states and actions. Among all those different value functions, we define the optimal value function as:<br>$$<br>\nu_*(s)=\max_\pi \nu_\pi(s),\forall s\in S<br>$$<br>which is actually the optimal state-value function. We also have the optimal action-value function as:<br>$$<br>q_*(s,a)=\max_\pi q_\pi(s,a),\forall s \in S, a \in A<br>$$<br>And they have the relationship:<br>$$<br>q_*(s,a)=\Bbb{E}\left[R(s,a)+\gamma\max_\pi\Bbb{E}\left[q_\pi(s’,a’)\right]\right]\<br>=\Bbb{E}\left[R(s,a)+\gamma\max_\pi\nu_\pi(s’)\right]\<br>=\Bbb{E}[R_t+\gamma\nu_*(S_{t+1})|S_t=s,A_t=a]<br>$$<br>Another relationship between the two is:<br>$$<br>\nu_*(s)=\max_{a\sim A}q_*(s,a)<br>$$<br>we can apply the Bellman equation on the pre-defined optimal value functions, which gives us the <strong>Bellman optimality equation</strong> (贝尔曼最优方程) , or called Bellman equation for optimal value functions, as follows.<br>$$<br>\nu_*(s)=\max_a\Bbb{E}<em>{s’\sim p(\cdot|s,a)}[R(s,a)+\gamma\nu</em><em>(s’)]<br>$$<br>Bellman equation for optimal action-value function is:<br>$$<br>q_</em>(s,a)=\Bbb{E}<em>{s’\sim p(\cdot|s,a)}[R(s,a)+\gamma \max</em>{a’}[q_*(s’,a’)]<br>$$<br> A policy with action sampled from the probability distribution is actually called the <strong>stochastic policy distribution</strong> (随机性策略分布) , with the action:<br>$$<br>a=\pi(\cdot|s)<br>$$<br>However, if we reduce the variance of the probability distribution of a stochastic policy and narrow down its range to the limit, we will get a Dirac delta function (δ function) as a distribution, which is the **deterministic policy **(确定性策略)  π(s). <strong>Deterministic policy π(s) also means given a state there is only one unique action as follows:</strong><br>$$<br>a\sim\pi(s)<br>$$<br>Note that the deterministic policy is no longer a mapping from a state and action to the conditional probability distribution, but rather a mapping from a state to an action directly. This slight difference will lead to some different derivations in the policy gradient method introduced in later sections.</p><p>As mentioned in previous sections, when the state in reinforcement learning environment is not fully represented by the observation for the agent, the environment is partially observable. For a Markov decision process, it is called the partially observed Markov decision process (POMDP), which forms a challenge for improving the policy without complete information of the environment states.</p><p>Summary of Terminology in Reinforcement Learning:</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220328200329.bmp"></p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器人 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>缺陷检测</title>
      <link href="2021/12/23/que-xian-jian-ce/"/>
      <url>2021/12/23/que-xian-jian-ce/</url>
      
        <content type="html"><![CDATA[<p>现在具体的项目还没看，但是老师基本说了一遍。大概就是注塑机的产品去用计算机视觉识别一下，然后判断出缺陷的程度种类大小，然后去自动调整注塑机的参数。整个流程是闭环的。</p><span id="more"></span><h1 id="1、基于深度学习的缺陷检测综述"><a href="#1、基于深度学习的缺陷检测综述" class="headerlink" title="1、基于深度学习的缺陷检测综述"></a>1、基于深度学习的缺陷检测综述</h1><blockquote><p><em>快速入门一个领域的方法就是阅读综述，我现在都先从中文文献入手来理清逻辑，语感好的可以直接看英文的综述。另外，还可以看看硕博的毕业论文，其中涉及到了更多的技术细节。</em></p></blockquote><ul><li>缺陷检测可以分为两类：有监督的方法——利用标记后的缺陷图像训练网络；无监督的方法——用正常无缺陷样本进行网络训练，当发现没有出现过的特征，则认为有异常[1]。</li><li>根据对缺陷的要求可以将检测的目标分成三个层次：1、缺陷是什么？给出图像的类别信息。2、缺陷在哪里？不仅给缺陷分类，还能给出缺陷的具体位置。3、缺陷是什么？将缺陷从背景中分割出来，得到缺陷的长度、面积、位置等信息。</li></ul><h2 id="1-1、有监督学习"><a href="#1-1、有监督学习" class="headerlink" title="1.1、有监督学习"></a>1.1、有监督学习</h2><p>全监督学习是主流。有监督的方法根据输入的形式可以分成表征学习和度量学习，表征学习类似CV中的分类任务，输入单张图片，然后对图片进行分类，而度量学习输入多张图片，然后根据图片相似度判断是否为一类。根据需要实现的三个层次的目标，可以将神经网络结构分为分类网络，检测网络和分割网络。</p><h3 id="分类网络"><a href="#分类网络" class="headerlink" title="分类网络"></a>分类网络</h3><ul><li>目前，缺陷分类中主要面临的问题是在真实工业场景下，由于检测大量对象时的差异，以及背景、光线、成像等复杂的环境使得分类要有很高的泛化性。在缺陷检测的分类任务中，主流应用的网络结构还是CNN。因为卷积核能有效提取局部特征值，所以基于CNN的网络结构应用十分广泛。使用CNN做分类可以直接使用原图，然后输出缺陷类型，但是由于一张图片中缺陷只有很小一部分，所以可以先对原图进行预处理，将缺陷部分裁剪以后在分类。当一张图像中存在多个缺陷时，可以设计双层网络，第一层进行是否存在缺陷的判断，第二层进行缺陷的分类。</li><li>针对定位任务，最简单的方法就是通过较小的滑动窗口来逐个处理，然后通过小窗口的结果来定位。但是这种方法造成计算量过大，所以可以通过热点图的方法（heatmap），通过图像分块和迁移学习的方法来获取每个分块的置信度。也可以将神经网络输出结果分叉，将一个分支作为分类，另一个分支作为定位。此外，也可以用CNN作为特征提取器，然后将提取得到的特征在输入到SVM分类器中。</li></ul><h3 id="检测网络"><a href="#检测网络" class="headerlink" title="检测网络"></a>检测网络</h3><ul><li>检测网络需要同时获得目标的准确位置以及类别信息。检测网络可以分成两类，一类是以fast-RCNN为代表的，两阶段网络模型，另一类则是以SSD和YOLO为代表的一阶段网络模型。两阶段的模型会有更高的精度（Xue等人[2]提出的改进fast-RCNN在盾构隧道中衬砌缺陷的检测中可以实现在每整图像48 ms的测试时间速度下, 实现检测精度超过95%；Ding等人[3]针对PCB提出的TDD-Net可在PCB缺陷数据集上达到了98.90%的mAP；He等人[4]等提出的基于Faster R-CNN的带钢表面缺陷检测网络在缺陷检测数据集NEU-DET上, 采用ResNet-50的backbone下实现了82.3%的mAP），而一阶段模型的会有更快的处理速度（Liu等人[5]采用基于MobileNet-SSD网络用于定位高铁接触网支撑组定位, 在测试的数据集上,其目标达到25fps检测速度和94.3%mAP）。</li><li>两阶段的模型是指第一阶段先用简单的神经网络进行初步的检测，然后在通过一个更复杂的网络来实现更精确的检测。而一阶段的模型则直接在输出层中来判断类别。这两种方法基本都有出现，并且在各种缺陷检测中都有较高的精度。</li></ul><h3 id="分割网络"><a href="#分割网络" class="headerlink" title="分割网络"></a>分割网络</h3><ul><li>分割是指将缺陷与正常区域分割出来，从而获得缺陷的位置、类别以及相应的几何属性。主要可以分为FCN和Mask R-CNN。FCN有可以分成FCN，U-Net和Seg-Net。Mask R-CNN在图像分割以及定位上有很好的效果，但是训练网络需要很长的时间。近年来的GAN也已经被广泛应用到各种缺陷检测中。分割网络得到的信息很多，但是由于精细分割是基于像素点的，所以标注时需要逐像素点进行标注，这个过程会花费大量的成本。</li></ul><h3 id="孪生网络"><a href="#孪生网络" class="headerlink" title="孪生网络"></a>孪生网络</h3><ul><li>不同于上述三种网络用单一图像作为输入，孪生网络用两张或者更多图片作为输入，通过训练网络参数使得相似类别的差距小，而不同类别的差距大。这种方法主要是度量学习的范畴，可近似看作是样本在特征空间进行聚类。孪生网络具体就是通过多张图片输入到共享权值的CNN中，然后通过相似度计算模型来计算损失值。这样需要的样本很少，也能达到较高的精度。但是，由于定位时需要图像具有统一的格式，所以工业中较少将孪生网络应用于定位。</li></ul><h2 id="1-2、无监督学习"><a href="#1-2、无监督学习" class="headerlink" title="1.2、无监督学习"></a>1.2、无监督学习</h2><p>用于表面缺陷检测的常见无监督方法主要是正常样本训练方法，即只用正常的样本（不包含缺陷）来训练网络。相比于全监督的方法，这种方法可以更容易检测到非正常的情况，甚至是未知的缺陷类别。该类方法主要可以分为两类。一类是自编码器，另一类是GAN（对抗神经网络神经）</p><h3 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h3><ul><li>采用自编码器主要是计算重建误差，然后通过不断训练网络来减少重建误差。输入是一个检测样本，然后输出是神经网络认为的正常样本，即重建成无缺陷的图像，然后通过计算重建误差来判断是否存在误差。图像中差异较大的就是缺陷所在位置。</li></ul><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><ul><li>通过训练GAN来生成类似正常样本的伪图像，GAN的判别器则用于分类缺陷还是正常样本。</li></ul><h2 id="1-3、弱监督和半监督学习"><a href="#1-3、弱监督和半监督学习" class="headerlink" title="1.3、弱监督和半监督学习"></a>1.3、弱监督和半监督学习</h2><p>弱监督学习与全监督学习相似，都是利用标签来训练网络，但是弱监督用弱标签来训练。半监督则是利用大量的未标记数据和少量的标记数据进行训练。</p><p><strong>参考文献：</strong></p><ol><li><a href="https://kns.cnki.net/kcms/detail/11.2109.TP.20200402.1101.002.html">陶显,侯伟,徐德.基于深度学习的表面缺陷检测方法综述[J/OL].自动化学报:1-19[2021-12-24]</a></li><li>Xue Y, Li Y. A fast detection method via region-based fully convolutional neural networks for shield tunnel lining de-fects. Computer-Aided Civil and Infrastructure Engineer-ing, 2018, 33(8): 638−654</li><li>Ding R, Dai L, Li G, Liu H. TDD-net: a tiny defect detec-tion network for printed circuit boards. CAAI Transactionson Intelligence Technology, 2019, 4(2): 110−116</li><li>He Y, Song K, Meng Q, Yan Y. An End-to-end Steel Surface Defect Detection Approach via Fusing Multiple Hierarchi-cal Features. IEEE Transactions on Instrumentation and Measurement, 2020, 69(4): 1493−1504</li><li>Liu Z, Liu K, Zhong J, Han Z, Zhang W. A High-Precision Positioning Approach for Catenary Support Components With Multiscale Difference. IEEE Transactions on Instru-mentation and Measurement, 2020, 69(3): 700−711</li></ol><p>传统方法综述<br>Luo Jing, Dong Ting-Ting, Song Dan, Xiu Chun-Bo. A review on surface defec detection. Journal of Frontiers of Computer Science and Technology, 2014, 8(9): 1041−1048 (in Chinese) (罗菁, 董婷婷, 宋丹, 修春波. 表面缺陷检测综述. 计算机科学与探索, 2014, 8(9): 1041−1048)</p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 缺陷检测 </tag>
            
            <tag> 人工智能 </tag>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Intro for DARTS-F</title>
      <link href="2021/10/13/intro-for-darts-f/"/>
      <url>2021/10/13/intro-for-darts-f/</url>
      
        <content type="html"><![CDATA[<blockquote><p><em>目前，多轴机械臂已经被大规模的应用在工厂中。在车企中使用机械臂进行焊接等工作已经十分常见，但是这种焊接等工作使用固定逻辑和框架，而不能使用客户定制化以及生产制造过程中柔性化的要求。所以，本文探讨了实现机器人柔性化的基本技能，即抓取。这里主要讨论的是最常用并且控制简单的二指平行机械爪，而不涉及到多指甚至人手型机械爪。本文通过深度学习的方法（具体指卷积神经网络）来实现机器人的智能抓取，并通过基于梯度的神经网络优化方法来优化网络结构，最后通过一个数字孪生框架提高神经网络的性能。</em></p></blockquote><h1 id="0-引言"><a href="#0-引言" class="headerlink" title="0.引言"></a>0.引言</h1><p>随着工业化的不断推进，以多轴式协作式机械臂为代表的机器人已经被广泛的应用在工业生产中。为完成复杂的柔性任务，提出了机器人智能抓取（Intelligent Robot Grasping，IRG），即采用一个手眼机器人系统自动抓取未知不同形状和颜色的物体，并放置到指定区域。该任务的核心是从包含物体颜色形状信息的图像或者数字模型中得到有效的抓取姿态。</p><p>由于CNN可以提取局部的特征，所以在2015年CNN（Convolutional Neural Network）刚提出的时候，简单的浅层CNN就用来预测抓取框。此时通过输入整张图像，然后分割成小图片分别进行预测。这种方法通过滑动小窗口来逐个预测，计算效率很低，因此2018年提出使用Heat-Map，即热点图，将抓取框的各种特征参数记录到各个每个图像像素中，来通过单一像素来预测一个抓取姿态，使得计算量减少了很多。</p><p>上述的改进主要是针对数据集形式或者数据集处理的改进，到现在更多的学者应用其他的新方法（比如强化学习，迁移学习）实现了更多有趣且复杂的任务。但就机器人抓取而言，改进网络结构可以极大的提升精度，同时使用深度卷积神经网络有过多的参数会导致计算速度偏慢而不能应用于工业场景下，因此，本文希望能将近些年刚出现的网络结构自动搜索（NAS）应用于机器人抓取任务上，来优化网络结构达到精度和速度平衡的CNN结构。</p><h1 id="1-任务定义"><a href="#1-任务定义" class="headerlink" title="1. 任务定义"></a>1. 任务定义</h1><p>这一节先描述了机器人抓取任务的场景，以及在此场景下，机器人抓取姿态是如何定义的，最后给出了在这个任务下神经网络输入和输出的形式，以及损失函数的定义。</p><h2 id="1-1-机器人抓取的定义"><a href="#1-1-机器人抓取的定义" class="headerlink" title="1.1 机器人抓取的定义"></a>1.1 机器人抓取的定义</h2><p>机器人抓取在工业中主要使用的是平行二指机械爪，这种末端执行器的抓取姿态可以通过一个矩形框来表示，如公式1所示，g表示的是在图像中抓取姿态的定义，括号中依次是笛卡尔坐标系下的位置，矩形框长边的角度，长边的宽度（只考虑长边是因为在短边对应执行器的参数不用作为变量），以及该姿态抓取成功的概率。</p><p>​                                                                                                    $$g = (p,  \theta, w, q)$$                                                                                  （1）</p><p>由于这种方法只考虑了抓取姿态在图像平面中的位置，而缺少三维的信息，因此只能用于简易的手眼机械臂系统中。但是在工业的流水线中，通过简易的手眼系统就可以实现大多数场景。我们将机器人抓取系统简化为如图（1）的情况——摄像头平面的法线垂直于物体放置的平面，然后将机械爪固定姿态来进行抓取（即垂直向下抓取）。通过简化生产场景可以直接用将图像中的抓取姿态通过坐标系转化得到物体在现实世界中的坐标，最终就将位置发给机械臂来进行抓取。</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220110221640.png" alt="Fig1"></p><h2 id="1-1-应用于IRG的CNN"><a href="#1-1-应用于IRG的CNN" class="headerlink" title="1.1 应用于IRG的CNN"></a>1.1 应用于IRG的CNN</h2><p>前言中说过由于可以有效提取局部特征，CNN被广泛应用于图像处理中，并且为了提出计算速度，学者们提出了使用热点图的方式来对每个像素进行预测。如图（2）所示，具体来说就是神经网络的输入是二维的图像（或者是包含深度图的三维图像），然后输出的结果是每个像素都可以代表一个抓取姿态的热点图($$G_{I} = \Theta_{I}, W_{I}, Q_{I}$$)。</p><p>$$\Theta_{I}, W_{I}, Q_{I}$$ 是三种相同尺寸的热点图。这三张热点图上的每个像素点的值分别代表一个抓取姿态，具体来说就是以该像素点为矩形框中心点 $$p$$ 时，该像素点在三类热点图上的像素值对应的公式（1）中的 $$\theta, w, q$$。 通过这种形式可以热点图中的每个像素都可以表示一个抓取姿态及其质量。</p><p>值得一提的是，我们通过增加一个维度来表示抓取姿态质量的高低，即$$q, quality$$ 就可以快速的确定神经网络预测的最优的抓取姿态。为了增加表示抓取姿态质量的维度，我们与之前的学者一样，在公开的数据集中进行处理。在2015年出现的康奈尔机器人抓取抓取数据集[（Cornell Robotic Grasping Dataset）](<a href="https://www.kaggle.com/oneoneliu/cornell-grasp">cornell_grasp | Kaggle</a>)，有800张RGB-D图像，是最早出现的数据集，虽然数据量不大，但是由于都是通过人工在图像上进行矩形框的选取，并且同一张图像中有认为可成功的抓取姿态，以及认为失败的抓取姿态。</p><p>因此，为了在像素层面上表示抓取姿态的质量，我们将康奈尔数据集中认为成功的矩形框中间的三分之一部分在 $$Q_{I}$$ 上置为1，表示对应这张图片的物体，该图像区域内任意矩形框都能成功抓取该物体。其余部分全部置为0。然后在其余两类热点图中记录这个矩形框的相关参数，这样就得到了我们训练网络时的对照组，也就是参考的基值（ground truth）。</p><p>自然而然的，我们神经网络的损失函数就定义为三类特征图的损失值的加权平均。</p><p><em>PS：一、实际中，这里损失值函数使用的是MSE；二、这里三类特征图的比例是1:1:1，我们尝试过不同的比例，比如1:1:2，2:1:3，但是效果没有明显的提升，或者这三类特征图之间的比例也可以用神经网络学习一下</em></p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220110221514.png" alt="Fig2"></p><h1 id="2-可微分式神经网络结构优化算法"><a href="#2-可微分式神经网络结构优化算法" class="headerlink" title="2. 可微分式神经网络结构优化算法"></a>2. 可微分式神经网络结构优化算法</h1><p>前言中说过，通过改进网络结构可以有效提高网络在机器人抓取任务上的表现。而通过人工设计网络一方面需要设计者有较高的深度学习基础，另一方面不能确保所设计的网络模型时最优的。因此，这里使用一种可微分式的神经网络结构优化算法来优化用于机器人抓取的神经网络。主要需要解决的问题有两点：一是如何将离散的网络结构用连续的变量来表示，并通过微分来计算最优解；二是应用于机器人抓取上的网络需要同时考虑精度和效率，神经网络结构并不是越大越好，如何解决精度和效率之间的矛盾的问题；三是高精度的机器人抓取实验系统需要较高的成本，能否设计仿真实验来指导实际实验。本节将依次解决这三个问题。</p><h2 id="2-1-搜索空间"><a href="#2-1-搜索空间" class="headerlink" title="2.1 搜索空间"></a>2.1 搜索空间</h2><p>首先我们需要明确的是我们在优化时哪些量是常量，哪些是变量。传统意义上，神经网络中的变量主要是所需要迭代计算的权重系数，而常量则是在训练前就已经通过经验确定下来的超参数，包含batch size，kernel size，channels，以及卷积层数量，池化层数量等等。神经网络结构优化就是指通过算法来计算之前用经验确定下来的超参数，即超参数不在是常量，而是变量。此外，由于batch size，epoch，或是激活层函数类型等等这些参数取不同的值也会对计算结构造成影响，但是这些属于是计算时数据处理的参数，结合计算硬件条件就可以基本确定，因此不再优化的范围内。我们的NAS主要考虑的是影响神经网络结构的超参数。</p><p>之前说过，由于卷积核能有效提取图像特征信息，因此我们的优化算法是基于卷积核的。当然NAS算法不仅限于CNN，关于RNN或者是ResNet等等的优化算法都有，我们这里只是用了简单的CNN网络，所以我们主要优化的就是卷积核的尺寸，卷积核的数量。</p><p>我们的优化算法流程如图（3）所示（参考了<a href="https://github.com/quark0/darts">DARTS</a>），图3中圆圈表示节点，与神经网络训练中的feature map概念相同。连接各个圆圈的是有向箭头表示不同的操作，操作就是卷积层，或者池化层，更细分的话是指卷积核尺寸为3×3的卷积层（图3中紫色的模型），卷积核尺寸为5×5的卷积核层（图3中橙色的模型），第一层卷积核尺寸3×3，第二层卷积核尺寸3×3（图3中紫色和绿色的模型）。所以优化算法的流程就是每个节点之间有很多不同的操作，然后通过优化算法找出各个节点之间最佳的操作，然后组成一个新的网络。</p><p>将离散的网络结构转化成连续的优化算法则是通过在不同的操作添加一个变量，来表示该操作对结果的影响。这个变量通过softmax可以转化为不同操作在计算中的占比。通俗来讲，就是给节点之间的每个操作都赋予一个概率系数 $$\alpha$$，这个系数表示了这个操作的计算结果在最终结果中占比是20%，还是50%。由于最终的损失函数值是不断变小的，因此，我们可以认为在计算过程中占比最多，概率系数最大的操作就是最合适的操作。然后将合适的操作提取出来，删除多余的操作，可以得到最终的离散网络，这个离散的网络性能会比连续下包含所有操作的网络性能更好，并且占用更少的计算。</p><p>下节将介绍优化算法的数学细节。</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220111002634.jpg" alt="Fig3"></p><h2 id="2-2-可微分式优化算法"><a href="#2-2-可微分式优化算法" class="headerlink" title="2.2 可微分式优化算法"></a>2.2 可微分式优化算法</h2><p>在定义搜索空间之后，需要确定具体的计算方式。参考<a href="https://github.com/quark0/darts">DARTS</a>提出的双层优化函数，他们将操作中的权重系数 $$(\omega)$$ 训练作为内层函数，将优化操作概率系数 $$(\alpha)$$ 作为外层函数。优化的迭代过程就是根据上一次迭代确定的权重系数更新操作系数，然后根据优化后的操作系数重新更新权重系数。如公式（2）和公式（3）所示，公式（2）是外层函数，公式（3）是内层函数。</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220110232405.png"></p><p>完全求解公式（3）意味着要讲包含所有可能操作的巨大神经网络训练到完全收敛，这一过程会耗费大量的时间，因此，DARTS的作者们提出了只更新一次内层函数然后在更新外层函数。他们不将操作中的权重系数完全收敛，然后通过只更新一次操作中的权重，如公式（5）所示，这样会大大减少计算所需要的时间，从而达到快速的优化的目的。这一策略在操作更多，节点数目更多的时候有着十分明显的效果（设想当一个有12种操作，有50个节点的网络，相当于约有600个卷积层）。</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220110232901.png"></p><p>但是这种策略一个主要的问题就是只更新一次内层函数的梯度，会造成在更新外层函数时梯度方向不明显，从而使得优化算法收敛时间变长，并且优化算法容易在错误的方向上达到局部最优解。而在工业上对响应时间会有很高要求，因此，实际应用在工业上不需要特别大特别深的卷积神经网络。而在这类小网络上，可以将内层函数多迭代几次直到近似收敛，然后再去更新外层函数，这样虽然牺牲了部分优化的计算速度，但是可以为外层优化神经网络结构的函数提供更明确的方向。我们的实验证明了在6个操作和3个节点上，6个操作和8个节点上都获得了更高的精度。</p><p>另外，为了在优化过程中考虑神经网络大小对计算速度的影响，我们在损失函数中引入了惩罚项，如公式（12）所示。该惩罚项中$$F(o_{kl})$$表示对$$o_{kl}$$操作计算其所需的浮点计算数（FLOPs）。整个惩罚项就是根据优化得到的概率系数$$\alpha$$得到最适合操作的FLOPs为整个损失函数增加一个值，这个值与FLOPs的大小成正比，因此优化算法在优化时，为了让损失函数最小，会平衡MSE与惩罚项，因而达到优化的网络结构同时考虑了精度和速度。</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220110235444.png"></p><p>最终我们实现了一个兼顾精度和速度的CNN结构优化算法，整个算法流程的伪代码如图（3）所示。下节将介绍一个数字孪生框架，来进一步的提高网络的性能，并通过仿真来减少实验所需的成本。</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220111000820.png"></p><h2 id="2-3-数字孪生框架"><a href="#2-3-数字孪生框架" class="headerlink" title="2.3 数字孪生框架"></a>2.3 数字孪生框架</h2><p>数字孪生是一个很大的概念，可以理解为工业生产中通过仿真结合实际数据来实时的更新产品的情况。这一技术十分重要，因为在现代的生产中产品的状态是很难通过其他办法去判断的，如果要停机检查则需要大量时间，并且不能立刻找到问题进行维护，所以需要通过数字孪生来构建产品的孪生体，随着产品的使用跟着实际的使用情况发生改变。其中涉及到很多细节，这里说数字孪生并不是完整的，而只是实现了其中一个小部分。</p><p>图（4）是用于机器人抓取的数字孪生框架示意图，我们将在康奈尔数据集上训练好的模型放到仿真平台中，然后通过计算仿真平台内得到的图像来计算预测的抓取姿态，然后在现实实验系统的数字孪生体中逐个进行测试，然后将成功抓取的矩形框记录下来，在康奈尔数据集训练的基础上重新训练，最终得到经过数字孪生框架提高的网络模型，将这个最终的模型应用到实际的实验系统来做最终的测试。图中中心位置红色的透明模型表示是优化结构后得到的网络模型（已经训练好权重系数）。图中左边是仿真平台，右边是实际的实验平台。</p><p>这两个实验记录在这个<a href="https://www.youtube.com/watch?v=qOtc4UtcUqE">视频</a>下。</p><p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220111002531.jpg" alt="Fig4"></p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器人 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本科科研经历分享</title>
      <link href="2021/03/04/ben-ke-ke-yan-jing-li-fen-xiang/"/>
      <url>2021/03/04/ben-ke-ke-yan-jing-li-fen-xiang/</url>
      
        <content type="html"><![CDATA[<p>在本科期间，我有幸接触了不同类型的导师以及很多在读的研究生，所以在这里将自己的经历以及各位学长的观点分享一下，希望对未来的自己和面临选择研究生导师的学弟学妹都能有帮助。本文章内容仅表示个人观点，仅供参考！</p><span id="more"></span><blockquote><p><em>注：我是机械工程专业，选导师每个专业和学院都不一样，请结合自身情况考虑</em></p></blockquote><h1 id="导师类型"><a href="#导师类型" class="headerlink" title="导师类型"></a>导师类型</h1><p>我将研究生导师按照年龄和职称分成三类，一类是年轻的青椒，这类老师刚进入高校，刚忍受完博士或者博士后生涯的苦逼人生，准备进入高校冲一冲一年只有一两个且逐年变少的教职，面对非升即走的压力，天天都在想着如何顺利完成考核指标，留在高校，实现财富（分房以及各类补贴）和时间（自己当老板，自己控制一切）双自由，从他们的角度出发不难看出他们的特点，那就是急于发论文拉项目，出成果完成考核指标。选这类老师作为研究生导师的好处就是你可以有很多出成果的机会，并且会参与很多项目，不过缺点是你有时会忙于项目无法集中精力在自己的研究甚至毕设上，至于实习就更别想了，并且你会很忙有做不完的事情。</p><p>另一类是年迈的老教授，这类导师基本功成名就，手底下有一大群的人，他们学术水平已经社会人脉都已经随着年份积累了很多，所以选他们作为导师的优点就是老师名声大，项目大且多，并且有很多人脉可以提供实习等各种，不过缺点就是他们手下人真的太多了，老师管不过来，同时这类老师其实早已不奋斗在技术一线上，实际指导你的可能是博士或者博后，所以如果你更看重研究生生涯的学术，你可能要了解一下有没有很强的学长，而不是去了解老师有多强。</p><p>最后一类就是介于两者之间，这类导师因人而异，他们没有考核的压力，但是也还没有足够大的名头，所以有些导师会开启正常的生活作息，像学校的管理层努力，或是办个企业当个上市集体老总，也有些导师会向着更高的学术进发，不断的接大项目不断的磨砺自己，最后成为院士，这类导师需要切实的了解他们的情况。如果可以联系到实验室的师兄师姐那么可以直接去问问导师怎么样，或者平时可以套磁几个感兴趣的老师多接触看看。还有推荐一下<a href="https://www.bilibili.com/video/BV1wC4y1x7c8">毕导的视频</a>，很形象的说明了各个导师的类型。</p><h1 id="个人情况"><a href="#个人情况" class="headerlink" title="个人情况"></a>个人情况</h1><p>我是本校保研至图形所学硕，目前还是大四。我其实一直是留学党，所以在本科期间尽量多参加了一些科研。大二跟制造所的一位发展很快的副教授（以下称为汪sir）做了国创（SRTP），参与了两项发明专利（已授权），一项挂名，另一项是主要的作者。虽然有一点点成果，但是项目完成后对这位副教授研究方向不是很感兴趣，所以大三找了当时出国交流的带队老师（以下称为老胡）去研究人工智能，跟着这位老胡参与组会做做科研一直到大四，毕设的导师也是他，现在有两篇国际会议，一篇期刊初稿。</p><h2 id="学术兴趣"><a href="#学术兴趣" class="headerlink" title="学术兴趣"></a>学术兴趣</h2><p>之前说到我是留学党，意向的国家是美国，所以我找的老师都是为了以后申请做准备。一开始想做的方向是机器人中柔性皮肤，所以大二一开始就跟制造所的老师做关于3D打印柔性传感器的项目，这个SRTP项目也是主动跟汪sir说后他才向学院申请，而不是那种老师缺人才申请的项目。在汪sir这里，主要做的事情就是设计个微流道然后用柔性的材料结合光固化打印出来，最后填充液态金属，其实最后的效果很差，但是发明专利并不要求很完美的实验结果，所以最后还是凭借比较新的设计授权了专利。另外值得一提的是，做国创的时候差不多是一个研一的没啥经验的学姐指导，做了将近一年和汪sir谈话的总时间不超过半小时吧。也因为这段经历，发现科研并没有想象中那种神秘或者高大，同时觉得这种换个材料，完善完善实验的方向没意思，所以之后就想着换老师。不过也凭借这段经历，之后大三暑研联系到一个国外直接发明液态金属的大牛，所以多些经历总是好事。<br><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/2021030401.png" alt="实验展示图"></p><p>在我想着跑路的时候，在大三暑假，学院组织学生出国交流，一个康奈尔回来的博后作为我们的带队老师，带我们去康奈尔进行了一周的访学，那时我们的带队老师是刚来学校一年不到的研究员。当时老师带着我们逛了校园，然后参观很多实验室，认识到很多大牛，比如发明ansys的实验室，以及一些做无人驾驶的实验室，还有做热能水利的教授。国外的教授喜欢研究对社会有重要影响的问题，这让人明白自己的工作是对社会有很大贡献的，这种感觉很激励人，所以大学里学术氛围挺不错，你可以随意找一个路人，问问他研究的是什么，大家都可以自信的介绍自己的研究。国内的博士也能自信的跟你说他研究的是啥，但是很少的人能自信的说出这个研究的意义。对我而言，读博是一种生活方式，而不是一个学历，如果没有足够的意义以及兴趣，我提不起一丝劲去花费大量的青春去过体验学术界最难的生活。</p><p>交流回来后，深受国外学术氛围影响，想出国读phd，于是想多增加一些计算机领域的科研经历，觉得我们的带队老师老胡还挺nice的，并且回国后他想搞机器人控制的优化，更重要的是他认识不少国外大老板，所以就联系他想去做科研，当时他刚来学校，无人可用，所以本科生也受到了欢迎，更重要的是他很支持学生出国，甚至会亲自写邮件帮本科生陶瓷，所以我们上一届的两个准备留学的学长申请的结果都挺好的。并且因为老胡刚来学校，学生比较少，所以刚开始进度抓的比较紧，每周六都要开组会，然后平时也会叫到实验室讨论。老胡本来的博士研究方向是风能优化，根据当前风能指标预测下一段指标，这个研究需要可靠的数据。回国后，老胡没有在美国时那种可靠的数据来源，所以他用自己的科研启动资金弄了个30w的协作机械臂，希望将优化理论结合到机器人上来，当时老师没其他项目，只是还在完成博后关于风能的工作，于是我跟老师商量了一下去研究自己感兴趣的机器人，所以那时上课之余天天花个三四个小时，天天都在想idea想着怎么写代码做实验都不感觉累。这段时间，老师就说好好做花时间肯定可以发论文出成果，等到有成果他帮我联系国外的教授要推荐信，说起来其实被老师画饼了，当时留学中介还提醒了一下。不过从现在的成果来说还过得去，只不过国外联系的教授没有一个建立长期的合作，更别提推荐信了。</p><p>经历这两段科研，我感觉科研没有多么高大，也没有什么意思。大家都是花时间看论文，然后想idea，最后实验验证一下，更重要的是有时候老师会为了申请基金的项目，强制你的研究跟什么不搭噶的方向扯一点。当然，不排除有些很强的老板学术热情很高，就是想着解决学术问题而奋斗的，这种老板对于有学术理想的学生还是挺适合的。我之前联系的那位发明液态金属的大牛就是这样，他是博士阶段在哈佛发明了液态金属，然后此后二十年一直在研究这种材料的相关性质以及各种创新的应用，所以如果你有足够强的科研热情，以及有不用担心前期柴米油盐的家庭条件，那我强烈推荐你继续科研。你的成果不仅仅能推动人类知识的发展，更重要的是你能实现很高的自我价值，因为知识无价更何况你的知识属于全人类。</p><h2 id="未来规划"><a href="#未来规划" class="headerlink" title="未来规划"></a>未来规划</h2><p>现在对我来说有两条路，一条路跟着老胡，被push三年然后发几篇论文，最后靠着他的关系申请个cornell的博士，然后重新开始更苦逼的藤校博士生涯，到30岁左右，回国开始卷高校教职，一直卷到35开始评青千等称号，之后看你还能不能更上一层。另一条路，就是换一个空闲一些的老师，自己规划时间准备转码也好，准备选调考公也行，总之不会面向学术界而是更多的向工业界求职。</p><p>这两条路有好有坏。第一条路好的就是跟着老胡确实能发论文，而且确实可以有cornell的校友推荐信，另外就是现在高校的教职也确实很香，分学区房子等等这种直接帮你解决了物质上问题就不提了，更重要你可以自己当老板，想研究啥就研究啥，想啥时候上班就什么时候上班，资金和时间都是自由分配，而且相比于商界的老板要考虑风险，你作为高校教授完全不用考虑风险，毕竟科研允许失败呀。不过坏处也很明显，你要一直在学术上卷，而学术发论文主要两种方式，一种蹭热点，另一种则是做到极致，蹭热点需要好的实验室硬件条件，而做到极致则需要你的脑子以及时间，还有很重要的运气。有很多开创性的研究其实都有不少人再做，但是人们最后记得只有第一名。像老胡这样的年轻老师，就是以蹭热点为主，将最新的理论应用到工程上，然后基于原先的理论根据工程相应的做些小创新，这样发论文很快，但是质量不是很高。质量不高的原因主要是这个理论仅仅适用于这个工程上。我还接触过一些控制算法的年轻老师，他们本质上是在玩数学，所以他们其实属于第二种，将一个事情做到极致，他们发论文数量就少很多，但是发的质量都很高。这种偏向理论可应用的领域就更多，自然而然引用更多，对应质量也越高。如果，你确定之后想去高校，那你确定了35岁之前都要干什么，那你就要早点开始想着自己核心竞争力到底是什么，可以是蹭热点的能力，实际上能力这种属于思维灵活，有很多创新想法的人，但是另一种则是沉下心打基础，然后做理论创新的，这种人在理科上很广泛，现在要求的计算机博士其实都想培养这样有理论深度的人，而不仅仅是应用的能力。除了核心竞争力外，一个博士还需要考虑你的论文写作能力，还有学术社交也是很重要的能力。对于我自己，我觉得在老胡这里的我的核心竞争力不强，说白了就是我发不出质量很高的论文，而且这个硕士需要三年，之后去国外读phd五年到八年，之后读完三十多岁了，完全没有后路，只能拼命卷，容错率很低，另一个就是自己觉得科研没有什么意思，相比于理论我更想创造一些有实用价值的东西。</p><p>在我看来，另一条路可以选择的就更多。利用你自己的时间可以准备转码多刷刷题，或者参加些学生活动准备选调。当然也有经验丰富的老博说你可以去个牛逼的老师读个一年，然后自己偷偷申请留学，有offer就退学。这样是为了可以减少你最终获得PHD的时间，使得未来还有的选择。反正看你最终目标是什么，那现在就可以规划起来。现在硕士的选择，我是从十年后35岁前后看，转码都说有个35岁优化的问题，这个问题真实存在，因为互联网需要不停的学习新技术，当年纪上去，学习能力肯定会有所下滑，所以优化也不可避免，但是这个问题也有很多解决方法。但现在来看，十年以后互联网只会比现在应用的更多，这个行业不会有缩减的趋势。另一方面，现在选调生确实太香了，我自己老爹就是公务员，感觉他现在五十岁左右的生活我还是有点点羡慕的，虽然他一直在说很累压力很大。不过不管二十年后的事情，我对自己的选择要求就是五年后不想后悔。五年能发生的事情太多了，多经历一些多体验一些，才能说活过吧</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><blockquote><p><em>1. 关于汪sir谈话时间少</em></p></blockquote><p>SRTP即大学生科研训练计划，项目重点旨在训练大二大三的本科生科研，所以老师也不会像对研究生一样花很多时间与本科生讨论，而是会让手下的研究生带着做点实验。我在汪sir那的谈话，都是主动联系老师进行讨论，只不过每次五分钟不到就被他问的答不上来了，只能灰溜溜的走了。</p><blockquote><p><em>2. 关于国内外学术氛围</em></p></blockquote><p>其实科研说白了两条路，一条路找热点，这条路发论文快，需要灵活的idea，并且实验速度一定要快，可以参考一下计算机领域，另一条路就是做到最好，这条路显然更难，但是重要性，以及成功以后的待遇都碾压第一条，不过由于实在太难，国内大多数组都是蹭热点阶段，大家不管这个工作意义或者自身兴趣，而是以能不能发文章作为导向。另外国内拉基金项目啥的其实也更看重人脉一些，这也导致一些想研究社会热点的学者得不到支持。不过全世界的大学都以发论文作为学术的评价指标，且所有国家的人都会因为人际关系有些偏向，在我看来，我们国家只不过决策层还不是很懂学术，这主要是现代科学发展较晚导致，不过只看论文数量不看论文质量近些年已经有些很大改变。</p><blockquote><p><em>3. 关于研究兴趣</em></p></blockquote><p>对我来说，我喜欢研究一些像机器人可以实际操作的设备。我不会从改变社会，为全人类做奉献这种宏大的目标出发做科研，我只想做点自己从心底喜欢的，比如做个外骨骼跟钢铁侠一样，可以预想这个兴趣如果做到极致，那也会实现全人类啥的这种很宏大的目标。不过科研做越深越难，越到后期越需要挖掘内在和外在的动力，而衡量这种挖掘的指标就是这个科研方向的潜力，如果在你读博前这个领域的问题已经基本解决了，那科研的目的好像只剩学历以及毕业以后的锅碗瓢盆了。为了学历而读博是最痛苦的。第一你的博士学历不一定能带给你很好的工作，第二你拿到博士学历时将近30岁，你如何面对只有学历没有其他一切的生活。</p><blockquote><p><em>4. 关于老胡画饼</em></p></blockquote><p>由于老胡刚回国是新接触机器人，所以他一开始能指导的也不多，当时他就说他认识了很多大牛，跟他好好做发个文章，给你推荐去大牛，所以当时拼命科研，而且只要有一点点成果他就说不错不错，过多久能发论文，那时他应该都没好好听过我的汇报，直到我有一定实验成果写了文章初稿以后，他把文章搁置了一个月然后花了一小时看了说不行，发不了论文。当时真的心态爆炸，直接跟他怼起来：你现在跟我说要改，为啥一开始做实验的时候不说，他一听也火了，说我学生态度有问题，那时候才明白老板很多时候都是敷衍，不过那文章实验确实没做好，所以后面就顺着老板给的台阶下了。另外，随着时间推荐，可以感觉到老胡的日常事务开始成指数型上升，他开始忙于申请各种奇奇怪怪的基金，马上他的工作重点就是做ppt给领导做汇报了，不过我作为本科生之后不留组，还不用承担项目（即帮他改ppt），其实他有一次让我们写个申请书，当时我就写的很差，然后把实验汇报了一大推，后来他就让我专心搞科研了。</p><blockquote><p><em>5. 关于保研</em></p></blockquote><p>今年因为疫情，所以托福考试啥的都抢不到考位，我最后放弃留学其一是因为英语不行，抢不到考位以及自身基础差，另外就是觉得保硕以后转码当个打工人也可以。我一直不想在国内读博，主要原因就是国内老师没有我特别喜欢做的方向，而且国内这几次科研经历都没有很好的感觉。</p>]]></content>
      
      
      <categories>
          
          <category> 人生 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 心得 </tag>
            
            <tag> 工作经历 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2021/03/03/hello-world/"/>
      <url>2021/03/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
