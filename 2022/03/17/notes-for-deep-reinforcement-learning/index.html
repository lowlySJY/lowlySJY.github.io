<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Notes for Deep Reinforcement Learning, Sanji">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Notes for Deep Reinforcement Learning | Sanji</title>
    <link rel="icon" type="image/jpeg" href="/logo.jpg">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"><link rel="stylesheet" href="/css/prism.css" type="text/css"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Sanji</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-file-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">Sanji</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-file-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/lowlySJY" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/lowlySJY" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Notes for Deep Reinforcement Learning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/">
                                <span class="chip bg-color">机器人</span>
                            </a>
                        
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">深度学习</span>
                            </a>
                        
                            <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">强化学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E7%A7%91%E7%A0%94/" class="post-category">
                                科研
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-03-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2022-03-28
                </div>
                

                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Fundamentals"><a href="#Fundamentals" class="headerlink" title="Fundamentals"></a>Fundamentals</h1><p>DL has a powerful data representation ability but it is not enough to build a smart AI system. This is because an AI system should not only able to learn from the provided data but also able to learn from interactions with the real world environment like a human. </p>
<p>RL is a subset of ML that enables computers to learn by interacting with the real world environment. In brief, RL separates the real world into two components—an environment and an agent. The agent interacts with the environment by performing specific actions and receives feedback from the environment. The feedback is usually termed as the “reward” in RL. The agent learns to perform “better” by trying to get more positive rewards from the environment. This learning process forms a feedback loop between the environment and agent, guiding the improvement of the agent with RL algorithms.</p>
<p>DRL is to combine the advantages of DL and RL for building AI systems. The main reason to use DL in RL is to leverage the scalability of DNN in high-dimensional space.</p>
<h2 id="Introduction-to-Deep-learning"><a href="#Introduction-to-Deep-learning" class="headerlink" title="Introduction to Deep learning"></a>Introduction to Deep learning</h2><p><strong>Discriminative Models</strong> study the conditional probability p(y|x) with input data x and a target label y. In other words, discriminative models predict the label y given the input data x. Discriminative models are mostly adopted in tasks such as classification and regression which require discriminative judgements.</p>
<p><strong>Generative Models</strong> are designed to study the joint probability p(x, y). Generative models are usually used to generate observed data by learning the distribution of the observed data.</p>
<h2 id="Introduction-to-Reinforcement-Learning"><a href="#Introduction-to-Reinforcement-Learning" class="headerlink" title="Introduction to Reinforcement Learning"></a>Introduction to Reinforcement Learning</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>The <strong>agent</strong> (智能体) and <strong>environment</strong> (环境) are the basic components of reinforcement learning.</p>
<p>The environment is an entity that the agent can interact with. An agent can “interact” with the environment by using a predefined <strong>action set</strong> <em>A<del>t</del></em> that describes all possible actions and get <strong>reward</strong> feedback by environment. The goal of reinforcement learning algorithms is to teach the agent how to interact “well” with the environment so that the agent is able to obtain a good score under a predefined evaluation metric.</p>
<p>At an arbitrary time step, <em>t</em>, the agent first observes the current state of the environment, <em>S<del>t</del></em> , and the corresponding reward value, <em>R<del>t</del></em> . The agent then decides what to do next based on the state and reward information. The action the agent intends to perform, <em>A<del>t</del></em> , gets fed back into the environment such that we can obtain the new state <em>S<del>t+1</del></em> and reward <em>R<del>t+1</del></em>. The observation of the environment state <em>s</em> (<em>s</em> is a general representation of state regardless of time step s) from the agent’s perspective does not always contain all the information about the environment. If the observation only contains partial state information, the environment is <strong>partially observable</strong>. Nevertheless, if the observation contains the complete state information of the environment, the environment is <strong>fully observable</strong>.</p>
<p>To provide feedback from the environment to the agent, a <strong>reward function</strong> <em>R</em> generates an <strong>immediate reward</strong> <em>R<del>t</del></em> according to the environment status and sends it to the agent at every time step. The reward function can depend on the current state only or the sequence of state-action pairs. In reinforcement learning, a <strong>trajectory</strong> (轨迹) is a sequence of states, actions, and rewards:<br>$$<br>\tau = (S_0, A_0, R_0, S_1, A_1, R_1,…)<br>$$<br>which records how the agent interacts with the environment. The initial state in a trajectory, <em>S<del>0</del></em>, is randomly sampled from the <strong>start-state distribution</strong>, denoted by <em>ρ<del>0</del></em>, in which:<br>$$<br>S_0 \sim \rho_0(\cdot)<br>$$<br>The transition from a state to the next state can be either a <strong>deterministic transition process</strong> or a <strong>stochastic transition process</strong>. For the deterministic transition, the next state St+1 is governed by a deterministic function:<br>$$<br>S_{t+1} = f(S_t, A_t)<br>$$<br>where a unique next state St+1 can be found. For the stochastic transition process, the next state St+1 is described as a probabilistic distribution:<br>$$<br>S_{t+1} \sim p(S_{t+1}|S_t, A_t)<br>$$<br>A trajectory, being referred to also as an <strong>episode</strong> (片段) , is a sequence that goes from the initial state to the terminal state (for finite cases).</p>
<p><strong>Exploitation</strong> (利用，也称为守成) means maximizing the agent performance using the existing knowledge, and its performance is usually evaluated by the expected reward. The <strong>greedy policy</strong> means the agent constantly performs the action that yields the highest expected reward based on current information, rather than taking risky trials which may lead to lower expected rewards.</p>
<p><strong>Exploration</strong> means increasing existing knowledge by taking actions and interacting with the environment. The <strong>exploration-exploitation trade-off</strong> describes the balance between how much efforts the agent makes on exploration and exploitation, respectively. The trade-off between exploration and exploitation is a central theme of reinforcement learning research and reinforcement learning algorithm development. </p>
<h3 id="Online-Prediction-and-Online-Learning"><a href="#Online-Prediction-and-Online-Learning" class="headerlink" title="Online Prediction and Online Learning"></a>Online Prediction and Online Learning</h3><p><strong>Online prediction problems</strong> are the class of problems where the agent has to make predictions about the future. Online prediction problems need to be solved with online methods. Online learning is distinguished from traditional statistic learning in the following aspects:</p>
<ul>
<li>The sample data is presented in an ordered sequence instead of an unordered batch.</li>
<li>We will often have to consider the worst case rather than the average case because we do not want things to go out of control in the learning stage</li>
<li>The learning objective can be different as online learning often tries to minimize regret whereas statistical learning tries to minimize empirical risk.</li>
</ul>
<p>The regret after n steps is defined as:<br>$$<br>R E_n = \left({\underset{j=1,2,…,K} {max}} {\sum_{t=1}^n}R_t^j\right)-{\sum_{t=1}^n}R_t^i<br>$$<br>The first term in the subtraction is the total reward that we accumulate until time <em>n</em> for always receiving the maximized rewards and the second term is the actual accumulated rewards in a trial that has gone through <em>n</em> time steps. In order to select the best action, we should try to minimize the expected regret because of the stochasticity introduced by our actions and rewards. We will differentiate two different types of regret, the expected regret and pseudo-regret (伪后悔值). The expected regret is defined as:<br>$$<br>\Bbb{E}[RE_n] = \Bbb{E}\left[({\underset{j=1,2,…,K} {max}} {\sum_{t=1}^n}R_t^j)-{\sum_{t=1}^n}R_t^i\right]<br>$$<br>The pseudo-regret is defined as:<br>$$<br>\overline {R E_n} = {\underset{j=1,2,…,K} {max}} \Bbb{E}\left[ {\sum_{t=1}^n}R_t^j-{\sum_{t=1}^n}R_t^i \right]<br>$$<br>The expected regret is harder to compute. This is because for the pseudo-regret we only need to find the action that optimizes the regret in expectation, however, for the expected regret, we will have to find the expected  regret that is optimal over actions across different trials. Concretely, we have $$\Bbb{E}[REn] ≥ \overline{REn}$$.</p>
<p>Stochastic MAB’s (Multi-Armed Bandit) reward functions are determined by the probabilistic distributions that are usually not changed. In reality, this might not be the case. Adversarial MAB modeling is needed when the rewards are no longer governed by a stationary probabilistic distributions but arbitrarily determined by some <strong>adversary</strong> (对抗者).</p>
<p>The general setup for adversarial MAB: At each time step, the agent will choose an arm <em>I<del>t</del></em> to pull and the adversary will decide the reward vector <em>R<del>t</del></em> for this time step. </p>
<p>We will call the adversary who sets the rewards independent of the past history an <strong>oblivious adversary</strong> (健忘对抗者) and the one that sets the rewards based on the past history a non-<strong>oblivious adversary</strong> (非健忘对抗者). We call the game in which a player has full knowledge of the reward vector a <strong>full-information game</strong> (全信息博弈) and the game with the knowledge of the reward for the action being played a <strong>partial-information game</strong> (部分信息博弈).</p>
<p>When the reward function for the task is stationary, we only need to find the best action, otherwise, when the task is non-stationary we will try to keep track of the changes.</p>
<h3 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h3><h4 id="Markov-Process-MP"><a href="#Markov-Process-MP" class="headerlink" title="Markov Process (MP)"></a>Markov Process (MP)</h4><p> A Markov process (MP) is a discrete stochastic process with Markov property, which simplifies the simulation of the world in continuous space. Figure 2.4 shows an example of MP. Each circle represents a state and each edge represents a state transition (状态转移)</p>
<p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220323163708.bmp" alt="Figure2.4"></p>
<p>In <strong>probabilistic graphical models</strong> (概率图模型) , specifically the ones that we use in this book, a circle indicates a variable, and the arrow with a single direction indicates the relationship between two variables.  For example, “a → b” indicates that variable b is dependent on variable a. The variable in a circle in white denotes a normal variable, while the variable in a circle with a shade of gray denotes an observed variable. A solid black square with variables inside indicates those variables are iterative</p>
<p>MP follows the assumption of <strong>Markov chain</strong> where the next state <em>S<del>t+1</del></em> is only dependent on the current state <em>S<del>t</del></em> , with the probability of a state jumping to the next state described as follows:<br>$$<br>p(S_{t+1}|S_t) = p(S_{t+1}|S_0, S_1, S_2,…, S_t)<br>$$<br>We also frequently use <em>s’</em> to represent the next state, in which the probability that state <em>s</em> at time <em>t</em> will lead to state <em>s’</em> at time <em>t + 1</em> is as following in a <strong>time-homogeneous</strong> Markov chain (时间同质马尔可夫链):<br>$$<br>p(s’|s) = p(S_{t+1}=s’|S_t=s)=p(S_{t+2}=s’|S_{t+1}=s)<br>$$<br><em>The time-homogeneous property is a basic assumption for most of the derivations in the book</em>, and we will not mention it but follow it as default in most cases. However, in practice, the time-homogeneous may not always hold, especially for non-stationary environments, multi-agent reinforcement learning, etc., which concerns with time-inhomogeneous/non-homogeneous cases.</p>
<p>Given a finite <strong>state set</strong> <em>S</em>, we can have a <strong>state transition matrix</strong> <em>P</em>. The <em>P</em> for Fig. 2.4 is as follows:</p>
<p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220323170109.bmp" alt="P"></p>
<p>where <em>P<del>i,j</del></em> represents the probability of transferring the current state <em>S<del>i</del></em> to the next state <em>S<del>j</del></em> . The sum of each row is equal to 1 and the <em>P</em> is always a square matrix. These probabilities indicate the whole process is stochastic. Markov process can be represented by a tuple of &lt; *S,P* &gt;. Mathematically, the next state is sampled from P as follows:<br>$$<br>S_{t+1} \sim \bf{P}_{S_t}<br>$$</p>
<h4 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h4><p><strong>Markov reward process</strong> (MRP) extends MP from &lt; *S,P* &gt; to &lt; *S,P, R, γ* &gt;. The R and γ represent the reward function and reward discount factor, respectively. The Fig. 2.6 and Fig. 2.7 show that the reward function depends on the current state in MRP:<br>$$<br>R_t = R(S_t)<br>$$<br> Given a list of immediate reward r for each time step in a single trajectory τ , a return is the cumulative reward of a trajectory:<br>$$<br>G_{t=0:T}=R(\tau)=\sum_{t=0}^T R_t<br>$$<br>where <em>R<del>t</del></em> is the immediate reward at time step <em>t</em>, and <em>T</em> represents the time step of the terminal state, or the total number of steps in a finite episode. For example, the trajectory (g, t<del>1</del>, t<del>2</del>, p, b) has an <strong>undiscounted return</strong> (非折扣化的回报) of 5 = −1 − 2 − 2 + 10.</p>
<p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220324131804.bmp" alt="Fig2.6&amp;2.7"></p>
<p>Often, the steps that are closer have a greater impact than the distant ones. We introduce the concept of <strong>discounted return</strong> (折扣化回报). The discounted return is a weighted sum of rewards which gives more weights to the closer time steps. We define the discounted return as follows:<br>$$<br>G_{t=0:T}=R(\tau)=\sum_{t=0}^T \gamma^tR_t<br>$$<br>where a reward discount factor γ ∈ [0, 1] is used to reduce the weights as the time step increases. . For example in Fig. 2.6, given γ = 0.9, the trajectory (g, t<del>1</del>, t<del>2</del>, p, b) has a return of 2.87 = −1 − 2 × 0.9 − 2 × 0.9^2 + 10 × 0.9^3</p>
<p>The <strong>value function</strong> (价值函数) <em>V (s)</em> represents the <strong>expected return (期望回报) from the state</strong> <em>s</em>.  If the agent acts according to the policy <em>π</em>, we denote the value function as <em>$$V^{π}(s)$$</em>:<br>$$<br>V^\pi(s)=\Bbb{E}_{\tau \sim \pi}[R{(\tau)}|S_0=s]<br>$$<br>A simple way to estimate the <em>V(s)</em> is <strong>Monte Carlo method</strong> (蒙特卡罗法), we can randomly sample a large number of trajectories starting from state s according to the given state transition matrix <em>P</em>. Take Fig. 2.6 as an example, given γ = 0.9 and P, to estimate <em>$$V^{π}(s=t_2)$$</em>,  we can randomly sample four trajectories as follows and compute the returns of all trajectories individually:<br>$$<br>s=(t_2,b),R=-2+0<em>0.9=-2\<br>s=(t_2,p,b),R=-2+10</em>0.9+0<em>0.9^2=7\<br>s=(t_2,r,t_2,p,b),R=-2+1</em>0.9-20.9^2+10<em>0.9^3+0=4.57\<br>s=(t_2,r,t_1,t_2,b),R=-2+0.9-2</em>0.9^2-2*0.9^3+0=-0.178<br>$$<br>Given the returns of all trajectories, the estimated expected return under state <em>s = t<del>2</del></em> is <em>V(s = t<del>2</del>)</em> = (−2 + 7 + 4.57 − 0.178)/4 = 2.348</p>
<h4 id="Markov-Decision-Process-MDP"><a href="#Markov-Decision-Process-MDP" class="headerlink" title="Markov Decision Process (MDP)"></a>Markov Decision Process (MDP)</h4><p>To model the process of sequential decision making, MDP is better than MR and MRP.  As Fig. 2.9 shows, different from MRP that the immediate rewards are conditioned on the state only (reward values on nodes), the immediate rewards of MDP are associated with the action and state (reward values on edges).</p>
<p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220324141411.bmp" alt="Fig. 2.9"></p>
<p>Given an action under a state, the next state is not fixed. For example, if the agent acts “rest” under state <em>s = t<del>2</del></em>, the next state can be either <em>s = t<del>1</del></em> or <em>t<del>2</del></em>. As mentioned above, MP can be defined as the tuple &lt; *S,P* &gt;, and MRP is defined as the tuple &lt; *S,P, R, γ* &gt;, where the element of state transition matrix is $$P_{s,s’} = p(s’ |s)$$. Here, MDP is defined as a tuple of &lt; *S, A,P, R, γ* &gt;. The element of state transition matrix becomes and Fig. 2.10 shows the graphical model of MDP in a probabilistic inference view.:<br>$$<br>p(s’|s,a) = p(S_{t+1}=s’|S_t=s,A_t=a)<br>$$<br><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220324144200.bmp" alt="Fig. 2.10"></p>
<p>For instance, most of the edges in Fig. 2.9 have a state transition probability of one, e.g., $$p(s’ = t_2|s = t_1, a = work) = 1$$, except that $$p(s’ |s = t_2, a = rest) = [0.2, 0.8]$$ which means if the agent performs action <em>a = rest</em> at state <em>s = t<del>2</del></em>, it has 0.2 probability will transit to state <em>s’ = t<del>1</del></em> , and 0.8 probability will keep the current state.</p>
<p>A <strong>policy</strong> <em>π</em> represents the way in which the agent behaves based on its observations of the environment. Specifically, the policy is a mapping from the each state s ∈ S and action a ∈ A to the probability distribution π(a|s) for taking action a in state s, where the distribution is:<br>$$<br>\pi(a|s)=p(A_t=a|S_t=s),\exist t<br>$$<br><strong>Expected return</strong> is the expectation of returns over all possible trajectories under a policy. Therefore, <strong>the goal of reinforcement learning is to find the higher expected return by optimizing the policy</strong>. Mathematically, given the start-state distribution <em>ρ<del>0</del></em> and the policy π, the probability of a T-step trajectory for MDP is:<br>$$<br>p(\tau|\pi)=\rho_0(S_0)\prod_{t=0}^{T-1}p(S_{t+1}|S_t,A_t)\pi(A_t|S_t)<br>$$<br>Given the reward function R and all possible trajectorites <em>τ</em>, the <strong>expected return</strong> J(π) is defined as follows:<br>$$<br>J(\pi)=\int_\tau p(\tau|\pi)R(\tau)=\Bbb{E}<em>{\tau\sim\pi}[R(\tau)]<br>$$<br>where <em>p</em> here means that the trajectory with higher probability will have a higher weight to the expected return. The <strong>RL optimization problem</strong> is to improve the policy for maximizing the expected return with optimization methods. The <strong>optimal policy</strong> π∗ can be expressed as:<br>$$<br>\pi^*=arg\max_\pi J(\pi)<br>$$<br>Given policy π, the <strong>value function</strong> <em>V(s)</em>, the expected return under the state, can be defined as:<br>$$<br>V^\pi(s)=\Bbb{E}</em>{\tau\sim\pi}[R(\tau)|S_0=s]\<br>        =\Bbb{E}<em>{A_t\sim\pi(\cdot|S_t)}\left[\sum</em>{t=0}^\infty\gamma^tR(S_t,A_t)|S_0=s\right]<br>$$<br>where τ ∼ π means the trajectories τ are sampled given the policy π, At ∼ π(·|St) means the action under a state is sampled from the policy, the next state is determined by the state transition matrix <strong>P</strong> given state s and action a.</p>
<p>In MDP, given an action, we have the <strong>action-value function</strong>, which depends on both the state and the action just taken. It gives an expected return under a state and an action. If the agent acts according to a policy π, we denote it as $$Q^π (s, a)$$, which is defined as:<br>$$<br>Q^\pi(s,a)=\Bbb{E}<em>{\tau\sim\pi}[R(\tau)|S_0=s,A_0=a]\<br>        =\Bbb{E}</em>{A_t\sim\pi(\cdot|S_t)}\left[\sum_{t=0}^\infty\gamma^tR(S_t,A_t)|S_0=s,A_0=a\right]<br>$$<br>We need to keep in mind that the $$Q^π (s, a)$$ depends on π, as the estimation of the value is an expectation over the trajectories by the policy π. This also indicates if the π changes, the corresponding $$Q^π (s, a)$$ will also change accordingly. We therefore usually call the value function estimated with a specific policy the <strong>on-policy value function</strong> (在线价值函数), for the distinction from the optimal value function estimated with the <strong>optimal policy</strong> (最优价值函数). We can observe the relation between vπ (s) and qπ (s, a):<br>$$<br>q_\pi(s,a)=\Bbb{E}<em>{\tau\sim\pi}[R(\tau)|S_0=s,A_0=a]\<br>\nu_\pi(s)=\Bbb{E}</em>{a\sim\pi}[q_\pi(s,a)]<br>$$<br>There are two simple ways to compute the value function $$v_π (s)$$ and action-value function $$q_π (s, a)$$: The first is the <strong>exhaustive method</strong> follows Eq. (40), it first computes the probabilities of all possible trajectories that start from a state, and then follows Eqs. (41) and (42) to compute the $$V ^π (s)$$ and $$Q^π (s, a)$$ for this state. However, in practice, the number of possible trajectories would be large and even infinite. Instead of using all possible trajectories, we can use <strong>Monte Carlo method</strong> as described in the previous MRP section to estimate the $$V^π (s)$$ by randomly sampling a large number of trajectories.</p>
<h4 id="Bellman-Equation-and-Optimality"><a href="#Bellman-Equation-and-Optimality" class="headerlink" title="Bellman Equation and Optimality"></a>Bellman Equation and Optimality</h4><p>The Bellman equation, also known as the Bellman expectation equation, is used to compute the expectation of value function given policy π, over the sampled trajectories guided by the policy. We call this <strong>“on-policy”</strong> manner as in reinforcement learning the policy is usually changing, and the value function is conditioned on or estimated by current policy</p>
<p>We can derive the Bellman equation for <strong>on-policy state-value function</strong> (在线状态价值函数) for MDP process in a recursive relationship:<br>$$<br>\nu_\pi(s)=\Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R(\tau_t:T)|S_t=s]\<br>=\Bbb{E}</em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R_t + \gamma R(\tau_{t+1}:T)|S_t=s]\<br>=\Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[r+\gamma\nu_\pi(s’)]<br>$$<br>the Bellman equation for MRP can be derived by simply removing the action from it:<br>$$<br>\nu(s)=\Bbb{E}</em>{s’\sim p(\cdot|s,a)}[r+\gamma\nu_(s’)]<br>$$<br>Bellman equation for **on-policy action-value function **(在线动作价值函数):<br>$$<br>q_\pi(s,a)=\Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R(\tau_t:T)|S_t=s,A_t=a]\<br>=\Bbb{E}</em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{T-1}R_T)|S_t=s,A_t=a]\<br>=\Bbb{E}<em>{S</em>{t+1}\sim\sim p(\cdot|S_{t},A_{t})}[R_{t}+\gamma \Bbb{E}<em>{a\sim\pi(\cdot|s),s’\sim p(\cdot|s,a)}[R(\tau</em>{t+1}:T)]|S_t=s]\<br>=\Bbb{E}<em>{S</em>{t+1}\sim\sim p(\cdot|S_{t},A_{t})}[R_{t}+\gamma \Bbb{E}<em>{A</em>{t+1}\sim\pi(\cdot|S_{t+1})}[q_\pi (S_{t+1},A_{t+1})]|S_t=s]\<br>=\Bbb{E}<em>{s’\sim p(\cdot|s,a)}[R(s,a)+\gamma \Bbb{E}</em>{a’\sim(\pi(\cdot|s’)}[q_\pi(s’,a’)]<br>$$<br>The above derivation is based on the finite MDP with maximal length of T , however, these formulas still hold when in the infinite MDP, simply with T replaced by “∞”. The two Bellman equations also do not depend on the formats of policy, which means they work for both stochastic policies π(·|s) and deterministic policies π(s).</p>
<p>The Bellman equation for MRP as in Eq. (47) can be solved directly if the transition function/matrix is known, which is called the <strong>inverse matrix method</strong> (逆矩阵方法). We rewrite Eq. (47) in a vector form for cases with discrete and finite state space as:<br>$$<br>\vec{\nu}=\vec{r}+\gamma\vec{P}\vec{\nu}<br>$$<br>where <strong>v</strong> and <strong>r</strong> are vectors with their elements <em>v(s)</em> and <em>R(s)</em> for all s ∈ S, and <strong>P</strong> is the transition probability matrix with elements p(s‘|s) for all s, s’ ∈ S. then<br>$$<br>\vec{\nu}=(I-\gamma\vec{P})^{-1} \vec{r}<br>$$<br>the complexity of the solution is O(n^3), where n is the number of states. Therefore this method does not work for a large number of states, meaning it may not be feasible for large-scale or continuous-valued problems. </p>
<p>Since on-policy value functions are estimated with respect to the policy itself, different policies will lead to different value functions, even for the same set of states and actions. Among all those different value functions, we define the optimal value function as:<br>$$<br>\nu_*(s)=\max_\pi \nu_\pi(s),\forall s\in S<br>$$<br>which is actually the optimal state-value function. We also have the optimal action-value function as:<br>$$<br>q_*(s,a)=\max_\pi q_\pi(s,a),\forall s \in S, a \in A<br>$$<br>And they have the relationship:<br>$$<br>q_*(s,a)=\Bbb{E}\left[R(s,a)+\gamma\max_\pi\Bbb{E}\left[q_\pi(s’,a’)\right]\right]\<br>=\Bbb{E}\left[R(s,a)+\gamma\max_\pi\nu_\pi(s’)\right]\<br>=\Bbb{E}[R_t+\gamma\nu_*(S_{t+1})|S_t=s,A_t=a]<br>$$<br>Another relationship between the two is:<br>$$<br>\nu_*(s)=\max_{a\sim A}q_*(s,a)<br>$$<br>we can apply the Bellman equation on the pre-defined optimal value functions, which gives us the <strong>Bellman optimality equation</strong> (贝尔曼最优方程) , or called Bellman equation for optimal value functions, as follows.<br>$$<br>\nu_*(s)=\max_a\Bbb{E}<em>{s’\sim p(\cdot|s,a)}[R(s,a)+\gamma\nu</em><em>(s’)]<br>$$<br>Bellman equation for optimal action-value function is:<br>$$<br>q_</em>(s,a)=\Bbb{E}<em>{s’\sim p(\cdot|s,a)}[R(s,a)+\gamma \max</em>{a’}[q_*(s’,a’)]<br>$$<br> A policy with action sampled from the probability distribution is actually called the <strong>stochastic policy distribution</strong> (随机性策略分布) , with the action:<br>$$<br>a=\pi(\cdot|s)<br>$$<br>However, if we reduce the variance of the probability distribution of a stochastic policy and narrow down its range to the limit, we will get a Dirac delta function (δ function) as a distribution, which is the **deterministic policy **(确定性策略)  π(s). <strong>Deterministic policy π(s) also means given a state there is only one unique action as follows:</strong><br>$$<br>a\sim\pi(s)<br>$$<br>Note that the deterministic policy is no longer a mapping from a state and action to the conditional probability distribution, but rather a mapping from a state to an action directly. This slight difference will lead to some different derivations in the policy gradient method introduced in later sections.</p>
<p>As mentioned in previous sections, when the state in reinforcement learning environment is not fully represented by the observation for the agent, the environment is partially observable. For a Markov decision process, it is called the partially observed Markov decision process (POMDP), which forms a challenge for improving the policy without complete information of the environment states.</p>
<p>Summary of Terminology in Reinforcement Learning:</p>
<p><img src="https://raw.githubusercontent.com/lowlySJY/imagesforblog/main/img/20220328200329.bmp"></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">jinyi</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://lowlySJY.github.io/2022/03/17/notes-for-deep-reinforcement-learning/">http://lowlySJY.github.io/2022/03/17/notes-for-deep-reinforcement-learning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">jinyi</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/">
                                    <span class="chip bg-color">机器人</span>
                                </a>
                            
                                <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">深度学习</span>
                                </a>
                            
                                <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">强化学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2022/03/17/notes-for-deep-reinforcement-learning/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/21.jpg" class="responsive-img" alt="Notes for Deep Reinforcement Learning">
                        
                        <span class="card-title">Notes for Deep Reinforcement Learning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Notes for reading 'Deep Reinforcement Learning' written by Dong et al.
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-03-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%A7%91%E7%A0%94/" class="post-category">
                                    科研
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA/">
                        <span class="chip bg-color">机器人</span>
                    </a>
                    
                    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                    <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">强化学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/12/23/que-xian-jian-ce/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="缺陷检测">
                        
                        <span class="card-title">缺陷检测</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            缺陷检测简单综述
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-12-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E9%A1%B9%E7%9B%AE/" class="post-category">
                                    项目
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B/">
                        <span class="chip bg-color">缺陷检测</span>
                    </a>
                    
                    <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">
                        <span class="chip bg-color">人工智能</span>
                    </a>
                    
                    <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">
                        <span class="chip bg-color">计算机视觉</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2021-2022</span>
            
            <span id="year">2021</span>
            <a href="/about" target="_blank">sjy</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/lowlySJY" class="tooltipped" target="_blank" data-tooltip="GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:S_jinyi@163.com" class="tooltipped" target="_blank" data-tooltip="email" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="17326089167" class="tooltipped" target="_blank" data-tooltip="Wechat: 17326089167" data-position="top" data-delay="50">
        <i class="fab fa-weixin"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1119510658" class="tooltipped" target="_blank" data-tooltip="QQ: 1119510658" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="/libs/background/canvas-nest.js"></script>
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
